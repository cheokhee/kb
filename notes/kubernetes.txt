kubectl get pods -A #-A: all namespace
kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}'
kubectl get pods -o wide
kubectl get namespace
kubectl get nodes
kubectl cluster-info
kubectl cluster-info dump
kubectl version --client
kubectl describe pods
kubectl describe service/kubernetes-bootcamp
kubectl logs <pod-name>
kubectl exec <pod_name> -- env
kubectl exec -ti <pod_name> -- /bin/bash
kubectl get events --sort-by=.metadata.creationTimestamp
kubectl get events --watch
kubectl run --replicas=100 my-web-server --image=my-web-app
kubectl scale --replicas=100 my-web-server
kubectl scale deployments/kubernetes-bootcamp --replicas=4
kubectl rolling-upgrade my-web-server --image=web-server:2
kubectl rolling-upgrade my-web-server --rollback
kubectl get service kubernetes-bootcamp --watch
kubectl top pods
kubectl get rs #get replicaset
kubernetes: API server, etcd, scheduler, kubelet, controller, container runtime

kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 #update image to version2
kubectl rollout status deployments/kubernetes-bootcamp
kubectl rollout undo deployments/kubernetes-bootcamp

kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}'

steps:
1. create cluster
2. create deployment
3. create service

#create deployment, a pod then gets created. then do kubectl get events to see what happened

#expose the pod
kubectl expose deployment hello-node --type=LoadBalancer --port=8080 #this is the port number inside the pod

kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1
kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4
kubectl get deployments

kubectl delete service hello-node
kubectl delete deployment hello-node

kubectl config set-context --current --namespace=<insert-namespace-name-here>

kubectl config view --minify | grep namespace:

kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false

kubernetes pod: a collection of containers co-located on a single machine. deployed as a unit.

kubernetes service: take one or more pods and expose them through a single endpoint URL.

kubernetes operator: a method of packaging, deploying, and managing a Kubernetes application.

kubectl -n dhmp-int-dshrd get ServiceBinding -l microservice=dhmp_ui -o name
kubectl -n dhxp-int-dshrd get VirtualService -l microservice=dhxp_svc -o name
kubectl apply -f k8s-acr-secret.yml --wait=true

kubectl -n e2ev-int-dshrd get Deployment -l microservice=e2ev_be -o name
kubectl -n e2ev-int-dshrd get Deployment e2ev-be-20200423191913819-deployment -o 'jsonpath={.metadata.labels.cms-replicas}'
kubectl -n e2ev-int-dshrd scale --replicas=1 deployment e2ev-be-20200423191913819-deployment
kubectl -n e2ev-int-dshrd wait --for=condition=available deployments e2ev-be-20200423191913819-deployment --timeout=180s

kubectl apply -f service-binding.yml --wait=true
kubectl apply -f my-app.yaml #Deploy the application
kubectl get service <app-name> --watch

Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual
clusters are called namespaces


https://kubernetes.io/docs/reference/kubectl/cheatsheet/
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/

kubernetes: A node pool is a group of nodes within a cluster that all have the same configuration.

Kubernetes coordinates a highly available cluster of computers that are connected to work as a
single unit. Kubernetes automates the distribution and scheduling of application containers across a
cluster in a more efficient way.

A Kubernetes cluster consists of two types of resources:
The Master coordinates the cluster
Nodes are the workers that run applications, Each node has a Kubelet, which is an agent for managing
the node and communicating with the Kubernetes master.

#create a proxy that will forward communications into the cluster-wide, private network
kubectl proxy #default port: 8001
curl http://localhost:8001/version
curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/

https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/
#1: using kubectl proxy
kubectl proxy --port=8080 &
curl http://localhost:8080/api/
#2: without proxy
# Gets the token value
TOKEN=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
# Explore the API with TOKEN
e.g. curl -X GET https://172.17.0.3:8443/api --header "Authorization: Bearer $TOKEN" --insecure
curl -X GET $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure

https://kubernetes.io/docs/concepts/overview/kubernetes-api/
#get swagger/openapi spec:
curl -X GET https://172.17.0.3:8443/openapi/v2 --header "Authorization: Bearer $TOKEN" --insecure

Once the application instances are created, a Kubernetes Deployment Controller continuously monitors
those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller
replaces the instance with an instance on another Node in the cluster. This provides a self-healing
mechanism to address machine failure or maintenance.

The kubelet is the primary “node agent” that runs on each node. It can register the node with the
apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud
provider.

Kubernetes Master is a collection of three processes that run on a single node in your cluster,
which is designated as the master node. Those processes are: kube-apiserver, kube-controller-manager
and kube-scheduler.
--
Each individual non-master node in your cluster runs two processes: kubelet, which communicates with
the Kubernetes Master.  kube-proxy, a network proxy which reflects Kubernetes networking services on
each node.

By default, the Pod is only accessible by its internal IP address within the Kubernetes cluster. To
make the hello-node Container accessible from outside the Kubernetes virtual network, you have to
expose the Pod as a Kubernetes Service.

kubernetes scheduler:
You create a pod
The scheduler notices that the new pod you created doesn’t have a node assigned to it
The scheduler assigns a node to the pod
It’s not responsible for actually running the pod – that’s the kubelet’s job. So it basically just
needs to make sure every pod has a node assigned to it.

Kubernetes in general has this idea of a “controller”. A controller’s job is to:
look at the state of the system
notice ways in which the actual state does not match the desired state (like “this pod needs to be assigned a node”)
repeat
The scheduler is a kind of controller. There are lots of different controllers and they all have different jobs and operate independently.

expose deployment type:
==
ClusterIP (default) - Exposes the Service on an internal IP in the cluster. This type makes the
   Service only reachable from within the cluster.
NodePort - Exposes the Service on the same port of each selected Node in the cluster using
   NAT. Makes a Service accessible from outside the cluster using <NodeIP>:<NodePort>. Superset of
   ClusterIP.
LoadBalancer - Creates an external load balancer in the current cloud (if supported) and assigns a
   fixed, external IP to the Service. Superset of NodePort.
ExternalName - Exposes the Service using an arbitrary name (specified by externalName in the spec)
   by returning a CNAME record with the name. No proxy is used. This type requires v1.7 or higher of
   kube-dns.


Control plane management:
==
etcd: is a lightweight and distributed cluster manager. It's persistent and reliably stores the
configuration data of the cluster, providing a consistent and accurate representation of the cluster
at any given point of time.
==
API server: serves the Kubernetes API using JSON over HTTP. It provides both an internal and
external interface to Kubernetes. The server processes and validates RESTful requests and enables
communication between and across several tools and libraries.
--
Scheduler: selects on which node an unscheduled pod should run. This logic is based on resource
availability. The scheduler also tracks resource utilization of each node, ensuring that the
assigned workload never exceeds what is available on the physical or virtual machine.
--
Control Manager: the process hosting the DaemonSet and Replication controllers. The controllers
communicate with the API server to create, update or delete managed resources.  Node management:
--
kubelet: responsible for the running state of each node and making sure that all containers on the
node are healthy. It handles the starting and stopping of application containers (see how this
differs with Docker in the next section) within a pod as directed by the manager in the control
plane.
--
kube-proxy: a network proxy and load balancer. It's responsible for routing traffic to the
appropriate container.  cAdvisor: an agent that monitors and collects system resource utilization
and performance metrics (such as CPU, memory, file and network) of each container on each node.
--
controller drives the state of the cluster by managing a set of pods. The Replication Controller
handles pod replication and scaling by running a specified number of copies of a given pod across
the entire cluster of nodes. It also can handle the creation of replacement pods in the event of a
failing node. The DaemonSet Controller is in charge of running exactly one pod per node. The Job
Controller runs pods to completion (that is, as part of a batch job).
--
In Kubernetes terms, a service consists of a set of pods working together (a one-tier or multi-tier
application). As Kubernetes provides service discovery and request routing (by assigning the
appropriate static networking parameters), it ensures that all service requests get to the right
pod, regardless of where it moves across the cluster. Some of this movement may be a result of pod
or node failure. In the end, Kubernetes' self-healing capabilities will get those ailing services
back to a pristine state automatically.

Pods
==
When a Kubernetes master deploys a group of one or more containers to a single node, it does so by
creating a pod. Pods abstract the networking and storage from the container, and all of the
containers within a pod will share the same IP address, hostname and more, allowing it to be moved
around in the cluster without complication.
--
The kubelet will monitor each and every pod. If it's not in a good state, it will redeploy that pod
to the same node. Apart from this, a heartbeat messaging mechanism will relay the node status to the
master every few seconds. As soon as the master detects a node failure, the Replication Controller
will launch the now affected pods onto another healthy node.
