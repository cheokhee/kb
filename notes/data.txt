Mode (statistics) The mode is the value that appears most often in a set of data.

mean: the average

The median is the value separating the higher half of a data sample, a population, or a probability
distribution, from the lower half. In simple terms, it may be thought of as the "middle" value of a
data set. For example, in the data set {1, 3, 3, 6, 7, 8, 9}, the median is 6, the fourth number in
the sample.

spread: how far from the center the data tend to range
standard deviation: the standard measure of spread/dispersion:
deviation: how much the data deviate from one another.

variance is the expectation of the squared deviation of a random variable from its mean,

standard deviation (SD, also represented by the Greek letter sigma or the Latin letter s) is a
measure that is used to quantify the amount of variation or dispersion of a set of data values.
population standard deviation vs sample standard deviation
if the data represents only a sample, use sample standard deviation

random variable: a variable that can take on any value from a set of values.
--> A Random Variable's set of values is the Sample Space.
--> a quantity whose value depends on some random events.
--> A Random Variable is a set of possible values from a random experiment.
discrete random variable: taking on a countable list of values
continuous random variable: taking any value in an interval
example: random variable X = the score on the face of a dice
X could be 1,2,3,4,5,6
so the sample space is {1,2,3,4,5,6}

dimension reduction is the process of reducing the number of random variables under consideration,
via obtaining a set of principal variables. It can be divided into feature selection and feature
extraction.

independent variable
dependent variable
nominal variable: have two or more categories
Dichotomous variables are nominal variables which have only two categories or levels
==
Ordinal variables are variables that have two or more categories just like nominal variables only
the categories can also be ordered or ranked
===
Interval variables are variables for which their central characteristic is that they can be measured
along a continuum and they have a numerical value (for example, temperature measured in degrees)
===
Ratio variables are interval variables, but with the added condition that 0 (zero) of the
measurement indicates that there is none of that variable. So, temperature measured in degrees
Celsius or Fahrenheit is not a ratio variable because 0C does not mean there is no temperature.

nominal variable: use mode
ordinal variable: use median
interval/ratio (not skewed): use mean
interval/ratio (skewed): use median

In machine learning, a target is called a label.
In statistics, a target is called a dependent variable.
A variable in statistics is called a feature in machine learning.
A transformation in statistics is called feature creation in machine learning.

In directed data mining, you are trying to predict a particular data point. eg. the sales price of a
house given information about other houses for sale in the neighborhood
==
In undirected data mining, you are trying to create groups of data, or find patterns in existing
data
--
data mining is about applying the right model to your data.

Regression is a data mining function that predicts a number. Age, weight, distance, temperature,
income, or sales could all be predicted using regression techniques.
regression: transforms existing data into a numerical prediction for future data

There are a number of independent variables, which, when taken together, produce a result - a
dependent variable. The regression model is then used to predict the result of an unknown dependent
variable, given the values of the independent variables.
==
The price of the house (the dependent variable) is the result of many independent variables - the
square footage of the house, the size of the lot, whether granite is in the kitchen, bathrooms are
upgraded, etc. So, if you've ever bought a house or sold a house, you've likely created a regression
model to price the house. You created the model based on other comparable houses in the neighborhood
and what they sold for (the model), then put the values of your own house into this model to produce
an expected price.

In statistics, linear regression is an approach for modeling the relationship between a scalar
dependent variable y and one or more explanatory variables (or independent variable) denoted X. The
case of one explanatory/independent variable is called simple linear regression.

regression: "How much should we charge for the new BMW M5?"
--
Regression models can answer a question with a numerical answer.
--
classification: "How likely is person X to buy the newest BMW M5?"
===
By creating a classification tree (a decision tree), the data can be mined to determine the
likelihood of this person to buy a new M5. Possible nodes on the tree would be age, income level,
current number of cars, marital status, kids, homeowner, or renter. The attributes of this person
can be used against the decision tree to determine the likelihood of him purchasing the M5.
===
Clustering: "What age groups like the silver BMW M5?"
==
The data can be mined to compare the age of the purchaser of past cars and the colors bought in the
past.

Nearest neighbor: "When people purchase the BMW M5, what other options do they tend to buy at the
same time?" (aka basket analysis: Market Basket Analysis is a modelling technique based upon the
theory that if you buy a certain group of items, you are more (or less) likely to buy another group
of items.)

Classification (also known as classification trees or decision trees) is a data mining algorithm
that creates a step-by-step guide for how to determine the output of a new data instance. The tree
it creates is exactly that: a tree whereby each node in the tree represents a spot where a decision
must be made based on the input, and you move to the next node and the next until you reach a leaf
that tells you the predicted output.

There are four measurement scales (or types of data): nominal, ordinal, interval and ratio.
--
Nominal scales are used for labeling variables, without any quantitative value. Nominal scales
could simply be called "labels". can also be called categories. examples: male/female.
--
ordinal: it is the order that matters
--
Interval scales are numeric scales in which we know not only the order, but also the exact
differences between the values: does not have a true zero. can add/subtract. cannot multiply/divide
--
ratio: they tell us about the order, they tell us the exact value between units, AND they also have
an absolute zero

Discrete attributes are those that describe a category, called nominal attributes. Those attributes
that describe a category that where there is a meaning in the order for the categories are called
ordinal attributes. The process of converting a real-valued attribute into an ordinal attribute or
bins is called discretization.
==
In statistics and machine learning, discretization refers to the process of converting or
partitioning continuous attributes, features or variables to discretized or nominal
attributes/features/variables/intervals.
--
Data binning or bucketing is a data pre-processing technique used to reduce the effects of minor
observation errors. The original data values which fall in a given small interval, a bin, are
replaced by a value representative of that interval, often the central value. It is a form of
quantization.
==
Statistical data binning is a way to group a number of more or less continuous values into a smaller
number of "bins". For example, if you have data about a group of people, you might want to arrange
their ages into a smaller number of age intervals.

null hypothesis: the hypothesis that there is no significant difference between specified
populations, any observed difference being due to sampling or experimental error.
===
The null hypothesis (H0) is a hypothesis which the researcher tries to disprove, reject or nullify.
==
The 'null' often refers to the common view of something, while the alternative hypothesis is what
the researcher really thinks is the cause of a phenomenon.
===
A null hypothesis is a type of hypothesis used in statistics that proposes that no statistical
significance exists in a set of given observations. The null hypothesis attempts to show that no
variation exists between variables or that a single variable is no different than its mean. It is
presumed to be true until statistical evidence nullifies it for an alternative hypothesis.
==
The null hypothesis, also known as the conjecture, assumes that any kind of difference or
significance you see in a set of data is due to chance.
==
A null hypothesis is a statistical hypothesis that is tested for possible rejection under the
assumption that it is true (usually that observations are the result of chance).
==
The hypothesis contrary to the null hypothesis, usually that the observations are the result of a
real effect, is known as the alternative hypothesis.
==
BEST: The null hypothesis, H0 is the commonly accepted fact; it is the opposite of the alternate
hypothesis. Researchers work to reject, nullify or disprove the null hypothesis. Researchers come up
with an alternate hypothesis, one that they think explains a phenomenon, and then work to reject the
null hypothesis.
==
The word null in this context means that it's a commonly accepted fact that researchers work to
nullify. It doesn't mean that the statement is null itself!
example: Null hypothesis, H0: The world is flat.

p-value
Measures the degree of linear relationship between two variables x and y. The correlation
coefficient assumes a value between âˆ’1 and +1.
==
A p-value indicates the size of an effect (e.g., strong evidence means big effect). A large p-value
means the null hypothesis is true, or provides evidence to support the null hypothesis. If the
p-value is small enough, the null hypothesis must be false.
==
The convention in most biological research is to use a significance level of 0.05. This means that
if the P value is less than 0.05, you reject the null hypothesis; if P is greater than or equal to
0.05, you don't reject the null hypothesis.

When you perform a hypothesis test in statistics, a p-value helps you determine the significance of
your results. Hypothesis tests are used to test the validity of a claim that is made about a
population. This claim thatâ€™s on trial, in essence, is called the null hypothesis.
==
The alternative hypothesis is the one you would believe if the null hypothesis is concluded to be
untrue. The evidence in the trial is your data and the statistics that go along with it. All
hypothesis tests ultimately use a p-value to weigh the strength of the evidence (what the data are
telling you about the population). The p-value is a number between 0 and 1 and interpreted in the
following way:
==
A small p-value (typically â‰¤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.
A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.
p-values very close to the cutoff (0.05) are considered to be marginal (could go either way). Always
report the p-value so your readers can draw their own conclusions.
==
compare the p-value to a significance level (usually 0.05), it acts as a cutoff point.
if p-value <= significance level, we rule out the null hypothesis and agree that something else is going on.

In general, in classification you have a set of predefined classes and want to know which class a new object belongs to.
Clustering tries to group a set of objects and find whether there is some relationship between the objects.
In the context of machine learning, classification is supervised learning and clustering is
unsupervised learning.

Classifiers in WEKA are the models for predicting nominal or numeric quantities

Association rules are if/then statements that help uncover relationships between seemingly unrelated
data in a relational database or other information repository. An example of an association rule
would be "If a customer buys a dozen eggs, he is 80% likely to also purchase milk."
==
Association rules analysis is a technique to uncover how items are associated to each other

The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution
of data based on the five number summary: minimum, first quartile, median, third quartile, and
maximum.
IQR (inter-quartile range) = third quartile - first quartile
===
The interquartile range (IQR) is a measure of variability, based on dividing a data set into
quartiles. Quartiles divide a rank-ordered data set into four equal parts. The values that divide
each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3,
respectively.

positively skewed (right-skewed): tail on the right is longer
negatively skewed (left-skewed): tail on the left is longer
